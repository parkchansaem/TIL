---
title: 5/26
date: "2022-05-26T22:40:32.169Z"
description: 공부내역
---

Logistic Regression: 주어진 데이터(X)를 통해서 사건의 발생 확률(y)를 예측하는 통계 모델

- 대표적인 이진 분류 모델
- 전체 데이터의 경향성을 파악하는 선형식으로 구하는 방법은 Linear Regression
- Linear Regressiond은 기본적으로 특정값을 예측하는 것이 목표
- Logistic regression은 선형회귀모델을 분류 모델로써 확장한 모델
- 특정한 카테고리를 예측하는데 적합
- Linear Regression 결과에 적당한 함수를 적용하여 output score를 0과 1사이의 값으로 변환하는 것으로 카테고리가 나올 확률을 예측하는 문제로 변환
- 이 확률값은 예측 값이 1이 될 확률이며, 이 확률이 0.5를 넘기면 1로 예측하고 그렇지 않다면 0으로 예측하는 분류 모델로써 사용가능

로지스틱 회귀분석

- 로지스틱 회귀 분석은 이진 분류를 수행하는데 사용된다. 즉 데이터 샘플을 1,0 클래스 둘 중 어디에 속하는지 예측한다
- 각 속성들의 계수 log-odds를 구한 후 sigmoid 함수를 적용하여 실제로 데이터가 해당 클래스에 속할 확률을 0과1사이의 값으로 나타낸다. 손실 함수는 머신러닝 모델이 얼마나 잘 예측하는지 확인하는 방법이다. 로지스틱 회귀의 손실 함수는 log loss이다.
- 데이터가 클래스에 속할지 말지 결정할 확률 컷오프 Threshold(임계값)이라 한다. 기본 값은 0.5이지만 데이터의 특성이나 상황에 따라 조정할 수 있다.
- 파이썬 라이브러리 Scikit-learn을 통해 모델을 생성하고 각 속성들의 계수를 구할 수 있다. 이때 각 계수들은 데이터를 분류함에 있어 해당 속성이 얼마나 중요한지 해석하는 데에 사용할 수 있다.

Decision Tree: 조건에 따라 데이터를 분류하는 모델

- 대표적인 non-parametric 모델
- 대표적인 white-box모델
- CART(classification and regression tree)
- Mini criterion은 불순도를 의미, 불순도란 불순한 정도 즉 섞여있는 정도를 의미, 즉 gini criteion이 0이 될 때가 깔끔하게ㅔ 나뉘어 잇는 경우
- 장점: 학습결과가 클리어하게 해석 가능, 학습이 쉽다.
- 단점: 쉽게 training data를 외우게 됨(오버피팅), training data가 조금만 바뀌어도 학습 모델이 전혀 다르게 만들어 짐

의사결정 나무의 한계

- 데이터를 먼저 나누어 더 좋은 트리ㄹ를 만들수 있는데 순간의 선택 때문에 가능성을 배제해버림, 최적의 트리를 찾는 건 매우 어려운 작업이고, 당장의 최적이라고 생각되는 것을 찾는 방식 또한 합리적이고 좋은 대안이라고 판단하기 때문에 사용
- 의사결정 나무는 학습 데이터에 오버피팅을 한다는 것, 이것을 해결하기 위해 크기를 줄여 가지치기를 사용

의사결정 나무

- 어떤 클래스에 속하는 데이터 포인트들이 동일한 레이블을 가질 때 순도가 높다고 표현, 순도 높은 이파리가 많아야 좋은 의사결정 나무
- 의사결정 나무는 현 상태에서 가장 높은 정보 획득량을 제공하는 속성을 우선적으로 선택하고 그 기준으로 데이터를 분할하는 탐욕알고리즘을 사용하여 트리를 만든다.
- 탐욕 알고리즘은 언제나 최적의 트리를 찾아주진 못하는 한계가 있다. 그러나 최적의 트리를 찾는 건 어려운 일
- 의사결정 나무는 오버피팅이 자주 발생, 가지치기를 하면 트리를 작게 만들어 일반화 할 수 있고 현실의 데이터에 적용했을 때 정확도를 높일 수 있음
